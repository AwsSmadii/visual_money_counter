{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad74fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.lib.utils import source\n",
    "from ultralytics.yolo.v8.detect.predict import DetectionPredictor\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "model = YOLO(r\"C:\\Users\\HP\\Desktop\\banknote_mini\\banknote_detect.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1/1: 0... Success  (inf frames of shape 640x480 at 30.00 FPS)\n",
      "\n",
      "\n",
      "    WARNING  stream/video/webcam/dir predict source will accumulate results in RAM unless `stream=True` is passed,\n",
      "    causing potential out-of-memory errors for large sources or long-running streams/videos.\n",
      "\n",
      "    Usage:\n",
      "        results = model(source=..., stream=True)  # generator of Results objects\n",
      "        for r in results:\n",
      "            boxes = r.boxes  # Boxes object for bbox outputs\n",
      "            masks = r.masks  # Masks object for segment masks outputs\n",
      "            probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "0: 480x640 1 twenty, 412.0ms\n",
      "0: 480x640 1 twenty, 382.0ms\n",
      "0: 480x640 1 twenty, 410.0ms\n",
      "0: 480x640 1 twenty, 393.0ms\n",
      "0: 480x640 1 twenty, 382.0ms\n",
      "0: 480x640 (no detections), 382.0ms\n",
      "0: 480x640 (no detections), 388.0ms\n",
      "0: 480x640 (no detections), 384.0ms\n",
      "0: 480x640 (no detections), 387.0ms\n",
      "0: 480x640 (no detections), 388.0ms\n",
      "0: 480x640 (no detections), 391.0ms\n",
      "0: 480x640 (no detections), 412.0ms\n",
      "0: 480x640 (no detections), 400.3ms\n",
      "0: 480x640 1 twenty, 399.0ms\n",
      "0: 480x640 1 twenty, 385.0ms\n",
      "0: 480x640 1 five, 1 twenty, 376.0ms\n",
      "0: 480x640 2 fives, 1 twenty, 373.0ms\n",
      "0: 480x640 2 fives, 384.0ms\n",
      "0: 480x640 1 five, 407.0ms\n",
      "0: 480x640 2 fives, 383.3ms\n",
      "0: 480x640 2 fives, 390.0ms\n",
      "0: 480x640 1 five, 389.0ms\n",
      "0: 480x640 2 fives, 383.0ms\n",
      "0: 480x640 2 fives, 410.0ms\n",
      "0: 480x640 (no detections), 386.0ms\n",
      "0: 480x640 (no detections), 412.0ms\n",
      "0: 480x640 (no detections), 418.0ms\n",
      "0: 480x640 2 fives, 396.0ms\n",
      "0: 480x640 1 one, 1 five, 393.0ms\n",
      "0: 480x640 1 one, 1 five, 409.0ms\n",
      "0: 480x640 1 one, 2 fives, 403.0ms\n",
      "0: 480x640 1 five, 381.0ms\n",
      "0: 480x640 2 fives, 391.0ms\n",
      "0: 480x640 1 one, 1 five, 395.0ms\n",
      "0: 480x640 1 one, 1 five, 411.0ms\n",
      "0: 480x640 1 five, 384.3ms\n",
      "0: 480x640 1 five, 410.3ms\n",
      "0: 480x640 1 five, 480.0ms\n",
      "0: 480x640 1 five, 391.0ms\n",
      "0: 480x640 3 fives, 396.0ms\n",
      "0: 480x640 1 five, 394.0ms\n",
      "0: 480x640 1 five, 397.0ms\n",
      "0: 480x640 (no detections), 389.0ms\n",
      "0: 480x640 (no detections), 391.0ms\n",
      "0: 480x640 1 five, 393.5ms\n",
      "0: 480x640 1 one, 1 five, 385.0ms\n",
      "0: 480x640 1 five, 377.0ms\n",
      "0: 480x640 (no detections), 388.0ms\n",
      "0: 480x640 2 fives, 384.0ms\n",
      "0: 480x640 2 fives, 385.0ms\n",
      "0: 480x640 1 five, 384.0ms\n",
      "0: 480x640 (no detections), 363.0ms\n",
      "0: 480x640 (no detections), 394.0ms\n",
      "0: 480x640 (no detections), 391.0ms\n",
      "0: 480x640 (no detections), 391.0ms\n",
      "0: 480x640 (no detections), 383.0ms\n",
      "0: 480x640 (no detections), 387.0ms\n",
      "0: 480x640 1 five, 392.0ms\n",
      "0: 480x640 1 five, 378.0ms\n",
      "0: 480x640 1 five, 409.0ms\n",
      "0: 480x640 1 five, 385.0ms\n",
      "0: 480x640 (no detections), 399.0ms\n",
      "0: 480x640 (no detections), 386.0ms\n",
      "0: 480x640 (no detections), 385.0ms\n",
      "0: 480x640 (no detections), 379.0ms\n",
      "0: 480x640 (no detections), 388.0ms\n",
      "0: 480x640 (no detections), 380.0ms\n",
      "0: 480x640 1 five, 386.0ms\n",
      "0: 480x640 1 five, 400.0ms\n",
      "0: 480x640 1 five, 397.0ms\n",
      "0: 480x640 (no detections), 375.0ms\n",
      "0: 480x640 (no detections), 390.0ms\n",
      "0: 480x640 (no detections), 390.0ms\n",
      "0: 480x640 (no detections), 390.0ms\n",
      "0: 480x640 1 five, 390.6ms\n",
      "0: 480x640 1 five, 399.0ms\n",
      "0: 480x640 1 five, 409.0ms\n",
      "0: 480x640 (no detections), 395.0ms\n",
      "0: 480x640 1 five, 378.0ms\n",
      "0: 480x640 (no detections), 394.0ms\n",
      "0: 480x640 (no detections), 397.4ms\n",
      "0: 480x640 2 fives, 406.0ms\n",
      "0: 480x640 1 five, 1 twenty, 387.0ms\n",
      "0: 480x640 1 five, 1 twenty, 392.0ms\n",
      "0: 480x640 1 five, 1 twenty, 411.0ms\n",
      "0: 480x640 1 five, 404.4ms\n",
      "0: 480x640 1 twenty, 400.0ms\n",
      "0: 480x640 1 five, 399.0ms\n",
      "0: 480x640 (no detections), 386.0ms\n",
      "0: 480x640 2 fives, 401.0ms\n",
      "0: 480x640 1 twenty, 415.0ms\n",
      "0: 480x640 1 five, 387.0ms\n",
      "0: 480x640 2 fives, 382.0ms\n",
      "0: 480x640 2 fives, 391.0ms\n",
      "0: 480x640 2 fives, 389.0ms\n",
      "0: 480x640 1 five, 1 twenty, 387.0ms\n",
      "0: 480x640 1 five, 1 twenty, 380.0ms\n",
      "0: 480x640 1 five, 1 twenty, 393.0ms\n",
      "0: 480x640 1 five, 1 twenty, 396.0ms\n",
      "0: 480x640 1 five, 390.0ms\n",
      "0: 480x640 2 fives, 390.0ms\n",
      "0: 480x640 1 five, 395.0ms\n",
      "0: 480x640 1 five, 397.0ms\n",
      "0: 480x640 1 five, 389.0ms\n",
      "0: 480x640 1 five, 383.0ms\n",
      "0: 480x640 1 five, 395.0ms\n",
      "0: 480x640 1 five, 392.0ms\n",
      "0: 480x640 (no detections), 395.1ms\n",
      "0: 480x640 (no detections), 365.0ms\n",
      "0: 480x640 (no detections), 384.0ms\n",
      "0: 480x640 (no detections), 393.0ms\n",
      "0: 480x640 1 five, 387.0ms\n",
      "0: 480x640 (no detections), 376.0ms\n",
      "0: 480x640 (no detections), 397.0ms\n",
      "0: 480x640 (no detections), 385.0ms\n",
      "0: 480x640 1 five, 375.0ms\n",
      "0: 480x640 1 five, 387.0ms\n",
      "0: 480x640 (no detections), 391.0ms\n",
      "0: 480x640 1 five, 386.0ms\n",
      "0: 480x640 (no detections), 393.0ms\n",
      "0: 480x640 1 five, 387.0ms\n",
      "0: 480x640 (no detections), 391.0ms\n",
      "0: 480x640 (no detections), 390.0ms\n",
      "0: 480x640 (no detections), 393.0ms\n",
      "0: 480x640 1 five, 391.0ms\n",
      "0: 480x640 1 five, 386.0ms\n",
      "0: 480x640 (no detections), 400.0ms\n",
      "0: 480x640 1 twenty, 387.0ms\n",
      "0: 480x640 1 twenty, 388.0ms\n",
      "0: 480x640 (no detections), 388.0ms\n",
      "0: 480x640 (no detections), 385.0ms\n",
      "0: 480x640 (no detections), 389.0ms\n",
      "0: 480x640 (no detections), 402.9ms\n",
      "0: 480x640 1 one, 415.0ms\n",
      "0: 480x640 1 one, 402.0ms\n",
      "0: 480x640 1 one, 389.0ms\n",
      "0: 480x640 (no detections), 389.0ms\n",
      "0: 480x640 1 one, 369.0ms\n",
      "0: 480x640 (no detections), 388.0ms\n",
      "0: 480x640 (no detections), 395.3ms\n",
      "0: 480x640 1 five, 378.0ms\n",
      "0: 480x640 (no detections), 391.0ms\n",
      "0: 480x640 (no detections), 404.0ms\n",
      "0: 480x640 (no detections), 413.0ms\n",
      "0: 480x640 1 one, 396.1ms\n",
      "0: 480x640 (no detections), 385.0ms\n",
      "0: 480x640 1 one, 393.0ms\n",
      "0: 480x640 1 one, 390.6ms\n",
      "0: 480x640 1 one, 394.0ms\n",
      "0: 480x640 1 one, 391.0ms\n",
      "0: 480x640 1 one, 386.0ms\n",
      "0: 480x640 1 one, 401.4ms\n",
      "0: 480x640 1 one, 391.0ms\n",
      "0: 480x640 (no detections), 385.0ms\n",
      "0: 480x640 1 one, 388.0ms\n",
      "0: 480x640 1 five, 389.0ms\n",
      "0: 480x640 (no detections), 401.4ms\n",
      "0: 480x640 (no detections), 389.0ms\n",
      "0: 480x640 (no detections), 385.0ms\n",
      "0: 480x640 1 five, 385.2ms\n",
      "0: 480x640 (no detections), 392.0ms\n",
      "0: 480x640 1 five, 396.0ms\n",
      "0: 480x640 1 one, 392.6ms\n",
      "0: 480x640 1 one, 395.0ms\n",
      "0: 480x640 1 one, 393.0ms\n",
      "0: 480x640 1 one, 398.0ms\n",
      "0: 480x640 1 one, 390.0ms\n",
      "0: 480x640 1 one, 392.0ms\n",
      "0: 480x640 1 one, 380.0ms\n",
      "0: 480x640 (no detections), 387.0ms\n",
      "0: 480x640 1 one, 393.0ms\n",
      "0: 480x640 1 one, 2 fives, 386.0ms\n",
      "0: 480x640 1 one, 1 five, 389.0ms\n",
      "0: 480x640 1 one, 1 five, 395.0ms\n",
      "0: 480x640 1 one, 1 five, 387.0ms\n",
      "0: 480x640 1 one, 1 five, 397.0ms\n",
      "0: 480x640 1 one, 1 five, 405.0ms\n",
      "0: 480x640 1 one, 1 five, 407.0ms\n",
      "0: 480x640 1 one, 1 five, 391.0ms\n",
      "0: 480x640 1 one, 1 five, 381.0ms\n",
      "0: 480x640 1 one, 1 five, 415.0ms\n",
      "0: 480x640 1 one, 1 five, 423.0ms\n",
      "0: 480x640 1 one, 1 five, 409.0ms\n",
      "0: 480x640 1 one, 1 five, 393.0ms\n",
      "0: 480x640 (no detections), 394.0ms\n",
      "0: 480x640 (no detections), 391.0ms\n",
      "0: 480x640 (no detections), 389.0ms\n",
      "0: 480x640 (no detections), 394.0ms\n",
      "0: 480x640 (no detections), 389.0ms\n",
      "0: 480x640 (no detections), 394.0ms\n",
      "0: 480x640 (no detections), 394.3ms\n",
      "0: 480x640 (no detections), 388.0ms\n",
      "0: 480x640 (no detections), 393.0ms\n",
      "0: 480x640 (no detections), 383.0ms\n",
      "0: 480x640 (no detections), 380.0ms\n",
      "0: 480x640 (no detections), 388.0ms\n",
      "0: 480x640 (no detections), 393.0ms\n",
      "0: 480x640 (no detections), 386.3ms\n",
      "0: 480x640 (no detections), 392.0ms\n",
      "0: 480x640 (no detections), 393.0ms\n",
      "0: 480x640 (no detections), 391.0ms\n",
      "0: 480x640 1 twenty, 363.0ms\n",
      "0: 480x640 (no detections), 389.0ms\n",
      "0: 480x640 (no detections), 392.0ms\n",
      "0: 480x640 (no detections), 384.0ms\n",
      "0: 480x640 (no detections), 390.0ms\n",
      "0: 480x640 (no detections), 366.0ms\n",
      "0: 480x640 (no detections), 387.1ms\n",
      "0: 480x640 (no detections), 399.0ms\n",
      "0: 480x640 (no detections), 375.0ms\n",
      "0: 480x640 (no detections), 385.0ms\n",
      "0: 480x640 (no detections), 398.0ms\n",
      "0: 480x640 (no detections), 397.0ms\n",
      "0: 480x640 (no detections), 371.0ms\n",
      "0: 480x640 (no detections), 385.0ms\n",
      "0: 480x640 (no detections), 391.0ms\n",
      "0: 480x640 (no detections), 397.0ms\n",
      "0: 480x640 (no detections), 390.0ms\n",
      "0: 480x640 (no detections), 413.0ms\n",
      "0: 480x640 (no detections), 393.0ms\n",
      "0: 480x640 (no detections), 385.0ms\n",
      "0: 480x640 (no detections), 398.0ms\n",
      "0: 480x640 (no detections), 391.0ms\n",
      "0: 480x640 (no detections), 365.2ms\n",
      "0: 480x640 (no detections), 395.0ms\n",
      "0: 480x640 (no detections), 376.0ms\n",
      "0: 480x640 (no detections), 400.0ms\n",
      "0: 480x640 (no detections), 383.9ms\n",
      "0: 480x640 (no detections), 394.8ms\n"
     ]
    }
   ],
   "source": [
    "results = model.predict(source='0',show=True,conf=0.6)\n",
    "if result = ['one','five','ten','twenty','fifty']:\n",
    "    cv2.putText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b05603b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'YOLO' object has no attribute 'seek'. See valid attributes below.\n\n    YOLO (You Only Look Once) object detection model.\n\n    Args:\n        model (str, Path): Path to the model file to load or create.\n        task (Any, optional): Task type for the YOLO model. Defaults to None.\n\n    Attributes:\n        predictor (Any): The predictor object.\n        model (Any): The model object.\n        trainer (Any): The trainer object.\n        task (str): The type of model task.\n        ckpt (Any): The checkpoint object if the model loaded from *.pt file.\n        cfg (str): The model configuration if loaded from *.yaml file.\n        ckpt_path (str): The checkpoint file path.\n        overrides (dict): Overrides for the trainer object.\n        metrics (Any): The data for metrics.\n\n    Methods:\n        __call__(source=None, stream=False, **kwargs):\n            Alias for the predict method.\n        _new(cfg:str, verbose:bool=True) -> None:\n            Initializes a new model and infers the task type from the model definitions.\n        _load(weights:str, task:str='') -> None:\n            Initializes a new model and infers the task type from the model head.\n        _check_is_pytorch_model() -> None:\n            Raises TypeError if the model is not a PyTorch model.\n        reset() -> None:\n            Resets the model modules.\n        info(verbose:bool=False) -> None:\n            Logs the model info.\n        fuse() -> None:\n            Fuses the model for faster inference.\n        predict(source=None, stream=False, **kwargs) -> List[ultralytics.yolo.engine.results.Results]:\n            Performs prediction using the YOLO model.\n\n    Returns:\n        list(ultralytics.yolo.engine.results.Results): The prediction results.\n    . You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\serialization.py:354\u001b[0m, in \u001b[0;36m_check_seekable\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 354\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m(f\u001b[38;5;241m.\u001b[39mtell())\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\ultralytics\\yolo\\engine\\model.py:116\u001b[0m, in \u001b[0;36mYOLO.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    115\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'YOLO' object has no attribute 'seek'. See valid attributes below.\n\n    YOLO (You Only Look Once) object detection model.\n\n    Args:\n        model (str, Path): Path to the model file to load or create.\n        task (Any, optional): Task type for the YOLO model. Defaults to None.\n\n    Attributes:\n        predictor (Any): The predictor object.\n        model (Any): The model object.\n        trainer (Any): The trainer object.\n        task (str): The type of model task.\n        ckpt (Any): The checkpoint object if the model loaded from *.pt file.\n        cfg (str): The model configuration if loaded from *.yaml file.\n        ckpt_path (str): The checkpoint file path.\n        overrides (dict): Overrides for the trainer object.\n        metrics (Any): The data for metrics.\n\n    Methods:\n        __call__(source=None, stream=False, **kwargs):\n            Alias for the predict method.\n        _new(cfg:str, verbose:bool=True) -> None:\n            Initializes a new model and infers the task type from the model definitions.\n        _load(weights:str, task:str='') -> None:\n            Initializes a new model and infers the task type from the model head.\n        _check_is_pytorch_model() -> None:\n            Raises TypeError if the model is not a PyTorch model.\n        reset() -> None:\n            Resets the model modules.\n        info(verbose:bool=False) -> None:\n            Logs the model info.\n        fuse() -> None:\n            Fuses the model for faster inference.\n        predict(source=None, stream=False, **kwargs) -> List[ultralytics.yolo.engine.results.Results]:\n            Performs prediction using the YOLO model.\n\n    Returns:\n        list(ultralytics.yolo.engine.results.Results): The prediction results.\n    ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m MODEL_PATH \u001b[38;5;241m=\u001b[39m model  \u001b[38;5;66;03m# Replace with the path to your trained model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load your PyTorch model\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Set up the webcam\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\serialization.py:276\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _open_buffer_writer(name_or_buffer)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_buffer_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in mode but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\serialization.py:261\u001b[0m, in \u001b[0;36m_open_buffer_reader.__init__\u001b[1;34m(self, buffer)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, buffer):\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(buffer)\n\u001b[1;32m--> 261\u001b[0m     \u001b[43m_check_seekable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\serialization.py:357\u001b[0m, in \u001b[0;36m_check_seekable\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (io\u001b[38;5;241m.\u001b[39mUnsupportedOperation, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 357\u001b[0m     \u001b[43mraise_err_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseek\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\serialization.py:350\u001b[0m, in \u001b[0;36m_check_seekable.<locals>.raise_err_msg\u001b[1;34m(patterns, e)\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[0;32m    347\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. You can only torch.load from a file that is seekable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    348\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please pre-load the data into a buffer like io.BytesIO and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    349\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m try to load from it instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 350\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(msg)\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'YOLO' object has no attribute 'seek'. See valid attributes below.\n\n    YOLO (You Only Look Once) object detection model.\n\n    Args:\n        model (str, Path): Path to the model file to load or create.\n        task (Any, optional): Task type for the YOLO model. Defaults to None.\n\n    Attributes:\n        predictor (Any): The predictor object.\n        model (Any): The model object.\n        trainer (Any): The trainer object.\n        task (str): The type of model task.\n        ckpt (Any): The checkpoint object if the model loaded from *.pt file.\n        cfg (str): The model configuration if loaded from *.yaml file.\n        ckpt_path (str): The checkpoint file path.\n        overrides (dict): Overrides for the trainer object.\n        metrics (Any): The data for metrics.\n\n    Methods:\n        __call__(source=None, stream=False, **kwargs):\n            Alias for the predict method.\n        _new(cfg:str, verbose:bool=True) -> None:\n            Initializes a new model and infers the task type from the model definitions.\n        _load(weights:str, task:str='') -> None:\n            Initializes a new model and infers the task type from the model head.\n        _check_is_pytorch_model() -> None:\n            Raises TypeError if the model is not a PyTorch model.\n        reset() -> None:\n            Resets the model modules.\n        info(verbose:bool=False) -> None:\n            Logs the model info.\n        fuse() -> None:\n            Fuses the model for faster inference.\n        predict(source=None, stream=False, **kwargs) -> List[ultralytics.yolo.engine.results.Results]:\n            Performs prediction using the YOLO model.\n\n    Returns:\n        list(ultralytics.yolo.engine.results.Results): The prediction results.\n    . You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Constants\n",
    "LABELS = ['one','five','ten','twenty','fifty']  # Replace with your own label names\n",
    "MODEL_PATH = model  # Replace with the path to your trained model\n",
    "\n",
    "# Load your PyTorch model\n",
    "model = torch.load(MODEL_PATH)\n",
    "model.eval()\n",
    "\n",
    "# Set up the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "denomination_sum = 0  # Initialize the sum variable\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert the frame to PIL Image format\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Preprocess the image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Adjust size according to your model's input requirements\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Adjust normalization values\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0)\n",
    "\n",
    "    # Forward pass through your model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "\n",
    "    # Get the predicted label\n",
    "    _, predicted_idx = torch.max(outputs, 1)\n",
    "    predicted_label = LABELS[predicted_idx.item()]\n",
    "\n",
    "    # Add denomination to the sum if the confidence is above a threshold\n",
    "    if outputs[0][predicted_idx] > 0.6:\n",
    "        denomination_sum += int(predicted_label[1:])\n",
    "\n",
    "    # Display the predicted label and sum on the frame\n",
    "    feedback_text = f\"Denomination: {predicted_label}\\nSum: ${denomination_sum}\"\n",
    "    cv2.putText(frame, feedback_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    # Show the frame with the predicted label and sum\n",
    "    cv2.imshow(\"Banknote Denomination\", frame)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd12cbf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'YOLO' object has no attribute 'load_state_dict'. See valid attributes below.\n\n    YOLO (You Only Look Once) object detection model.\n\n    Args:\n        model (str, Path): Path to the model file to load or create.\n        task (Any, optional): Task type for the YOLO model. Defaults to None.\n\n    Attributes:\n        predictor (Any): The predictor object.\n        model (Any): The model object.\n        trainer (Any): The trainer object.\n        task (str): The type of model task.\n        ckpt (Any): The checkpoint object if the model loaded from *.pt file.\n        cfg (str): The model configuration if loaded from *.yaml file.\n        ckpt_path (str): The checkpoint file path.\n        overrides (dict): Overrides for the trainer object.\n        metrics (Any): The data for metrics.\n\n    Methods:\n        __call__(source=None, stream=False, **kwargs):\n            Alias for the predict method.\n        _new(cfg:str, verbose:bool=True) -> None:\n            Initializes a new model and infers the task type from the model definitions.\n        _load(weights:str, task:str='') -> None:\n            Initializes a new model and infers the task type from the model head.\n        _check_is_pytorch_model() -> None:\n            Raises TypeError if the model is not a PyTorch model.\n        reset() -> None:\n            Resets the model modules.\n        info(verbose:bool=False) -> None:\n            Logs the model info.\n        fuse() -> None:\n            Fuses the model for faster inference.\n        predict(source=None, stream=False, **kwargs) -> List[ultralytics.yolo.engine.results.Results]:\n            Performs prediction using the YOLO model.\n\n    Returns:\n        list(ultralytics.yolo.engine.results.Results): The prediction results.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 12\u001b[0m\n\u001b[0;32m      7\u001b[0m LABELS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfive\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mten\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwenty\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfifty\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Replace with your own label names\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# MODEL_PATH = \"path/to/your/model.pt\"  # Replace with the path to your trained model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load your YOLO model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# model = YourYOLOModel()  # Replace with your actual YOLO model instantiation\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m(torch\u001b[38;5;241m.\u001b[39mload(model))\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Set up the webcam\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\ultralytics\\yolo\\engine\\model.py:116\u001b[0m, in \u001b[0;36mYOLO.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m\"\"\"Raises error if object has no requested attribute.\"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'YOLO' object has no attribute 'load_state_dict'. See valid attributes below.\n\n    YOLO (You Only Look Once) object detection model.\n\n    Args:\n        model (str, Path): Path to the model file to load or create.\n        task (Any, optional): Task type for the YOLO model. Defaults to None.\n\n    Attributes:\n        predictor (Any): The predictor object.\n        model (Any): The model object.\n        trainer (Any): The trainer object.\n        task (str): The type of model task.\n        ckpt (Any): The checkpoint object if the model loaded from *.pt file.\n        cfg (str): The model configuration if loaded from *.yaml file.\n        ckpt_path (str): The checkpoint file path.\n        overrides (dict): Overrides for the trainer object.\n        metrics (Any): The data for metrics.\n\n    Methods:\n        __call__(source=None, stream=False, **kwargs):\n            Alias for the predict method.\n        _new(cfg:str, verbose:bool=True) -> None:\n            Initializes a new model and infers the task type from the model definitions.\n        _load(weights:str, task:str='') -> None:\n            Initializes a new model and infers the task type from the model head.\n        _check_is_pytorch_model() -> None:\n            Raises TypeError if the model is not a PyTorch model.\n        reset() -> None:\n            Resets the model modules.\n        info(verbose:bool=False) -> None:\n            Logs the model info.\n        fuse() -> None:\n            Fuses the model for faster inference.\n        predict(source=None, stream=False, **kwargs) -> List[ultralytics.yolo.engine.results.Results]:\n            Performs prediction using the YOLO model.\n\n    Returns:\n        list(ultralytics.yolo.engine.results.Results): The prediction results.\n    "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Constants\n",
    "LABELS = ['one','five','ten','twenty','fifty']  # Replace with your own label names\n",
    "# MODEL_PATH = \"path/to/your/model.pt\"  # Replace with the path to your trained model\n",
    "\n",
    "# Load your YOLO model\n",
    "# model = YourYOLOModel()  # Replace with your actual YOLO model instantiation\n",
    "model.load_state_dict(torch.load(model))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Set up the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "denomination_sum = 0  # Initialize the sum variable\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert the frame to PIL Image format\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Preprocess the image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((416, 416)),  # Adjust size according to your YOLO model's input requirements\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0)\n",
    "\n",
    "    # Forward pass through your YOLO model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "\n",
    "    # Process the outputs and get the predicted labels\n",
    "    # (implement the logic specific to your YOLO model here)\n",
    "\n",
    "    # Add denomination to the sum if the confidence is above a threshold\n",
    "    # (implement the logic specific to your YOLO model here)\n",
    "\n",
    "    # Display the predicted label and sum on the frame\n",
    "    # (implement the logic specific to your YOLO model here)\n",
    "\n",
    "    # Show the frame with the predicted label and sum\n",
    "    cv2.imshow(\"Banknote Denomination\", frame)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5456e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
